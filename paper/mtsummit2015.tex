\documentclass[]{article}
\usepackage[letterpaper]{geometry}
\usepackage{mtsummit2015}
\usepackage{times}
\usepackage{hyperref}
\usepackage{latexsym}
\usepackage{natbib}
\usepackage{layout}
\usepackage{multirow}
\usepackage[dvipsnames]{xcolor}  % Coloured text etc.
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\usepackage{listings}
\usepackage{color}
\usepackage{caption}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[nolist,nohyperlinks]{acronym}
\usepackage{textcomp}
\usetikzlibrary{shapes,arrows}

\newcommand{\confname}{AMTA 2018}
\newcommand{\website}{\protect\url{https://sites.google.com/view/loresmt/}}
\newcommand{\contactname}{research track co-chair Yaser Al-Onaizan}
\newcommand{\contactemail}{chaohong.liu@adaptcentre.ie}
\newcommand{\conffilename}{mtsummit2015}
\newcommand{\downloadsite}{\protect\url{http://www.conference.amtaweb.org/}}
\newcommand{\paperlength}{$8$ (eight)}
\newcommand{\shortpaperlength}{$4$ (four)}

%% do not add any other page- or text-size instruction here

\parskip=0.00in

\begin{document}

\begin{acronym}
\acro{NMT}{Neural Machine Translation}
\acro{MT}{Machine Translation}
\acro{UNMT}{Unsupervised Machine Translation Using Monolingual Corpora Only}
\acro{LSTM}{Long Short Term Memory}
\end{acronym}

% \mtsummitHeader{x}{x}{xxx-xxx}{2015}{45-character paper description goes here}{Author(s) initials and last name go here}
\title{\bf System description of Supervised and Unsupervised  \acl{NMT} approaches from {"NL~Processing"} team at DeepHack.Babel task}
\author{\name{\bf Ilya Gusev} \hfill  \addr{ilya.gusev@phystech.edu}\\
        \addr{MIPT, Dolgoprudny, Moscow Region, 141701, Russian Federation}
\AND
        \name{\bf Artem Oboturov} \hfill \addr{oboturov@gmail.com}
}

\maketitle
\pagestyle{empty}

\begin{abstract}
A comparison~\footnote{Images used to train models are publicly available at \url{https://github.com/aoboturov/loresmt-nlprocessing}} of supervised and unsupervised \ac{NMT} models was done for the corpora provided by the DeepHack.Babel competition.
It is shown that for even small parallel corpus, fully supervised \ac{NMT} gives better results than fully unsupervised for the case of constrained domain of the corpus.
We have also implemented a fully unsupervised and a semi-supervised \ac{NMT} models which have not given positive results compared to fully supervised models.
A blind set-up is described where participants know at no point what language pair is used for translation, so no extra data could be integrated in pre-submission phase or during training.
Finally, future competition organizers should find ways to protect their competition set-ups against various attacks in order to prevent from revealing of language pairs.
We have reported two possible types of attacks on the blind set-up.
\end{abstract}

\acresetall
\section{Competition Set-up}
\label{sect:set_up}

The work presented here is motivated by the following observation: an industrial \ac{NMT} system is usually built on a huge parallel corpora and trained for days or even weeks.
A "raw" \ac{NMT} model is then tuned by additional training on client-specific data and by augmentation with some domain-specific information.
What if it is not as important to have such a heavy and difficult-to-train model?
Instead, why not just use a simple bootstart model based only on the client's data with a subsequent augmentation done using unsupervised learning, which would use any available non-parallel corpora?
If such approach would produce results comparable to models trained on large parallel corpora, one could significantly reduce costs of preparing parallel corpora and instead focus on better unsupervised models which work with non-parallel corpora (which are much easier and cheaper to produce).
It might also help for the case of low resource languages when no large parallel corpora exist.
This paper attempts to answer these questions.

We present the results obtained by the "NL~Processing" team in the DeepHack.Babel hackathon~\footnote{The leaderboard: \url{http://contest.deephack.me/c/babel/leaderboard}} on semi-supervised machine translation.
Organizers of the competition created a blind set-up - a case in which the source and target languages are not known at any stage of the competition and the machine translation system should be trained with no specific tuning to the language pair.
Language pairs were trained and scored independently, so no one sought to build a universal model.
Training and translation were performed by the scoring system.
Participants have no insight into the process and could only observe the final score for a submission and/or the failure status.
For each language pair participants can submit multiple entries and, based on the scores, adapt their models.
Submissions were scored in BLEU-4~\citep{papineni2002bleu}.
The fact that the language pair was not known should have prevented participants from any specific tuning and pre-training of their submitted models; for each submitted model, it had a strict time limit for training and inference (8 hours in total for both stages) and a computational budget constrained by a single dedicated GPU.
The participants' models were not allowed to access the internet in the training and the evaluation process.

For each language pair the following datasets~\footnote{Contest overview: \url{http://contest.deephack.me/c/babel/overview}} were available:
\begin{itemize}
  \item each language of a pair has one monolingual corpora, {\tt 1M} sentences;
  \item a small parallel corpus, {\tt 50K} sentence pairs;
  \item and an input corpus to be translated from source to target language, {\tt 6K} sentences.
\end{itemize}

There were 3 language pairs used during the competition: {\tt En-Ru} for test, {\tt Lv-En} for qualification and {\tt En-Ko} for final scoring.
Data for training and test are not available publicly and organizers would not release them.
Therefore we could only provide a summary~\footnote{Samples from the parallel corpora are provided online \url{https://github.com/aoboturov/loresmt-nlprocessing\#the-corpora-extracts}}: Table~\ref{table:corpora_stats} describes statistics for the corpora.

\begin{table}[h!]
\begin{center}
\begin{tabular}{ l c c c c }
Pair & Source Tokens & Source Words & Target Tokens & Target Words \\
\hline
En-Ru & 14M & 165519 & 19M & 345444 \\
Lv-En & 21M & 502858 & 24M & 341012 \\
En-Ko & 14M & 157649 & 7M & 530124 \\
\end{tabular}
\end{center}
\caption{Descriptive statistics for the corpora.}
\label{table:corpora_stats}
\end{table}

The machine translation system could be built as a fully supervised one, though the parallel corpus is small ({\tt 50K}); as an unsupervised one, using the two monolingual corpora; and as a semi-supervised one.
Given the problem at hand, a simple fully supervised \ac{NMT} baseline was implemented which was then compared against the \ac{UNMT} model which was trained both in fully unsupervised and semi-supervised modes.

To prepare for the {{DeepHack.Babel}} hackathon we looked into recent supervised \ac{NMT} systems~\footnote{One could find them on-line: \url{https://github.com/aoboturov/aoboturov-deephack-babel-qualification}} including: Google's seq2seq~\citep{Britz:2017}, {FAIR Sequence-to-Sequence Toolkit}~\citep{gehring2017convs2s}, {{Marian-NMT}}~\citep{junczys2016neural} and Sockeye~\citep{Sockeye:17}.
For the competition, however, we focused on the theme of the hackathon, which was on unsupervised and semi-supervised models under the conditions of the blind set-up.

In Section~\ref{sect:baseline} we outline the baseline that was used to benchmark the \ac{UNMT} in the blind set-up.
In Section~\ref{sect:unmt} we discuss our experiments with the \ac{UNMT} model for the blind set-up.
Finally, in Section~\ref{sect:non_blind} we investigate whether prior knowledge of a language pair gives an advantage for the unsupervised \ac{NMT} approach.

\section{Baseline}
\label{sect:baseline}

A supervised \ac{NMT} model~\footnote{For a full description of the Encoder-Decoder architecture see \url{https://github.com/aoboturov/loresmt-nlprocessing\#supervised-model-description}.} was chosen for the baseline.
The model was implemented in OpenNMT~\citep{opennmt} and had the following Encoder-Decoder architecture:
\begin{itemize}
\item encoder is a continuous embedding from the source language to a $300$ dimensional space;
\item followed by a $3$-layer \acs{LSTM} with a dropout;
\item the decoder has a stacked \acs{LSTM} with a dropout, a global attention \citep{luong2015effective}, and a continuous embedding from a $300$ dimensional space.
\end{itemize}

For each language pair a model was trained only on a {\tt 50K} parallel corpus with a {\tt 5\%} validation set.
Training on an NVIDIA Titan XP GPU usually lasted for $20$ to $30$ minutes, the results of which are provided in Table~\ref{table:results}.
Additionally, embeddings were trained with Fasttext~\citep{bojanowski2016enriching}.
We have a number of different combinations of \acs{LSTM} depths and cell-sizes, but we did not search for optimal hyper parameters for the supervised baseline.
We have realized that, even without optimal hyperparameters, the baseline beats the \ac{UNMT} score by an order of magnitude.

On the {\tt Lv-En} language pair, the model performance was mediocre.
This could be explained by the fact that {\tt En-Ru} and {\tt En-Ko} were topic-restricted corpora - both were related to tourism only, while the {\tt Lv-En} corpora was extracted from a news feed which had no topic constraints.

\section{Unsupervised Neural Machine Translation}
\label{sect:unmt}

The competition included not only parallel corpus for each language pair, but also {\tt 1M} monolingual corpus for each language.
One way to leverage this data is to use unsupervised \ac{NMT} model described in \cite{DBLP:journals/corr/abs-1711-00043}.
The code for this model is not available, so we built our own implementation~\footnote{Implementation of the \acs{UNMT}: \url{https://github.com/IlyaGusev/UNMT}} based on the PyTorch~\citep{paszke2017automatic} framework.
One can train this model on monolingual corpora using a predefined initial model, which we refer to in this paper as the zero model.
The goal of the competition was to find unsupervised and semi-supervised \ac{MT} methods applicable in practice.
A fully unsupervised case is covered in Section~\ref{sect:fully_unsupervised}, while a semi-supervised approach is described in Section~\ref{sect:semi_unsupervised}.

The \ac{UNMT}~\footnote{The \ac{UNMT} model used for translation is described in \url{https://github.com/aoboturov/loresmt-nlprocessing\#unmt-model-description}} would train iteratively using adversarial training~\citep{goodfellow2016nips} with a discriminator~\footnote{The Discriminator description is available online \url{https://github.com/aoboturov/loresmt-nlprocessing\#unmt-discriminator-description}}.
In both the semi-supervised and unsupervised cases we ran an unsupervised training epoch which starts from a batch of sentences translated by a model from a previous iteration of unsupervised training (or zero model if it is the first iteration) followed by a noising layer and a pass through the model that has been trained on the current iteration.
Figure~\ref{fig:unsupervised_training} gives a graphical explanation of the training process.

\begin{figure}
\begin{tikzpicture}[auto, thick, node distance=2cm, >=triangle 45]
\draw;

\end{tikzpicture}
\caption{Unsupervised epoch training for the \ac{UNMT} model.}
\label{fig:unsupervised_training}
\todo[inline]{ao}{Diagram from Figure 2 bottom of the UNMT paper}
\end{figure}

There are two types of zero models which we have used: dictionary translation, and a supervised model trained on a small corpora.
The translation model has an RNN Encoder-Decoder architecture \citep{DBLP:journals/corr/ChoMGBSB14} with continuous embeddings and a global attention \citep{luong2015effective}.

\subsection{\ac{UNMT} with a dictionary translation zero model}
\label{sect:fully_unsupervised}
The dictionary translation model is a translation process which uses a dictionary obtained with an unsupervised embedding~\citep{conneau2017word} (or otherwise an external dictionary could have been used if the language pair was known) to translate each sentence word by word.

To debug the zero model we first check the input to output copy which is reported in Table~\ref{table:results} as the {\tt In to Out Copy} result.
Normally, we would expect an improvement over the {\tt In to Out Copy}, because it is closely related to dictionary translation: words which are not in the dictionary would be copied over from source to target sentences.

\todo{ao}{Run word-by-word separately to see the impact}

We do not know whether there is an error in our system or the corpora are too small.
BLEU scores on all language pairs were below {\tt 0.01 BLEU}.

\subsection{\ac{UNMT} with a fully supervised zero model}
\label{sect:semi_unsupervised}
\todo{ao}{update this section with the results I have obtained myself running the simulations}

\section{Prior Language Pair Information}
\label{sect:non_blind}

In this section we describe how the blind set-up can be hacked to improve our results, given that the competition is structured so that the participants do not know the language-pairs being used, and it would be difficult to determine these language pairs within the scope of the competition.
The hackathon could have had any pair-combination of $42$ languages supported by {{Booking.com}}, so the total number of models, if trained unidirectionally, would have been over $1500$.
Given that even for our simplest baseline model an individual \ac{NMT} model is at least $300$~Megabytes, we would have had to train individual unidirectional models, likely for over several days, on some external parallel data, which we did not have for all language pairs, and to package around half a Terabyte of data inside a docker container, which is technologically unrealistic.
We could have followed Google's \ac{NMT} approach~\citep{johnson2016google} or any other \ac{MT} approach, which have intermediate neural representation, to reduce the total number of models to just one, it should still have to be trained on external parallel corpora, even if they all would be just English to any other of the $41$ languages.
To reduce the combinatorial complexity of the problem, one could potentially identify the language pair and then just train a single unidirectional model.
The competition testing system prevented the access to any external resources and remote calls during training and inference phases.
The sheer size of model representations, the total training time, amount and diversity of training data and technical constraints would make pre-training a non-viable option.
The only information available to participants was the BLEU score and the failure or success status for the submission.
With these information, however, one could devise at least two attacks to identify the language pair and then using this prior knowledge, use it to construct a better translation algorithm.

In Table~\ref{table:results} we reported the best BLEU scores available within the conference submissions for each of the pairs trained on common corpora.
On the one hand, we could see that a margin of improvement is just a couple of BLEU points for {\tt En-Ru} and {\tt En-Ko} pairs.
On the other hand, {\tt Lv-En} has a very poor result and we would expect that both unsupervised learning and prior knowledge may improve this score.

Below, we describe at least two ways how a language pair identification Side-Channel attack could be executed.
The execution time attack is supposed to identify the language pair in a single submission, while the second one would require multiple submissions.
The number of submissions used to identify the language pair would matter when the total number of submissions for the competition is limited.

\subsection{Using Execution Time}
\label{sect:execution_time_attack}

There is a way to identify the language pair in one submission by using the side-channel attack technique.
In this particular case, the side-channel would be the execution time of the translation algorithm whereby a language identification routine is run on each of the non-parallel corpora and both languages of the pair are detected.
Given that the routine could identify $N$ languages, all the pairs could be enumerated to define a mapping of natural numbers in the range $1\dots N*(N-1)$.
Provided that a specific constant delay is used, one could divide the total execution time by the delay duration to obtain the index of the pair in the mapping.

\subsection{Using Failure Status}
\label{sect:failure_status_attack}

The second way is a slower combinatorial way in which a failure status is used as an indicator of the language belonging to a subset of languages being tested.
A set of all languages identifiable by the routine could be searched in log-time in a breadth-search fashion descending only into subsets where we have established an inclusion relationship.

\section{Results}
\label{sect:results}

\begin{table}
\begin{center}
\begin{tabular}{ l c c c }
Model & En-Ru Score & Lv-En Score & En-Ko Score \\
\hline
Supervised, 10 Epochs & 0.28915 & 0.05756 & 0.25418 \\
In to Out Copy & 0.02123 & 0.02075 & 0.02759 \\
Unsupervised UNMT & - & 0.00428 & - \\
Semi-supervised UNMT & - & - & 0.03632 \\
Competitors Best & - & 0.23338 & 0.30074 \\
Literature Best, conscious & 0.29800 & 0.22900 & 0.27950
\end{tabular}
\end{center}
\caption{Evaluation results for models in the blind set-up, measured in BLEU scores.}
\label{table:results}
\end{table}

In Table~\ref{table:results}, the first four models are the ones that we have produced for the competition, followed by the result reported by the winning team for each round.
The last model is reported from literature reviews for the conscious set-up.
The best {\tt Lv-En} and  {\tt En-Ru} are from the newstest2017 corpora in \cite{bojar2017findings}.
% Lv-En \url{http://matrix.statmt.org/matrix/output/1872?score_id=22981}
%\url{http://matrix.statmt.org/matrix/output/1875?score_id=21229}
{\tt En-Ko} is reported from the work of \cite{junczys2016coppa}, which uses COPPA corpus.
The Literature Best models provide an indicative benchmark on what a \ac{MT} system trained on a generic parallel corpus might score on a translation task when the language pair is known.

A generic fully unsupervised machine translation problem is hard.
In some cases, one could obtain good machine translation models by having a small data set for a limited domain, e.g. for a case of traveling destinations or some other domain-specific translation.
Although semi-supervised translation might improve the results, we have not observed that a fully supervised model used as the zero model for the \ac{UNMT} made any translation improvement over a regular supervised model.
Poor performance of the \ac{UNMT} has to be investigated further, possibly by providing larger non-parallel corpora and changing \ac{UNMT} model architecture.

\small

\bibliographystyle{apalike}
\bibliography{mtsummit2015}

\end{document}
