\documentclass[]{article}
\usepackage[letterpaper]{geometry}
\usepackage{mtsummit2015}
\usepackage{times}
\usepackage{hyperref}
\usepackage{latexsym}
\usepackage{natbib}
\usepackage{layout}
\usepackage{multirow}
\usepackage[dvipsnames]{xcolor}  % Coloured text etc.
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\usepackage{listings}
\usepackage{color}
\usepackage{caption}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\newcommand{\confname}{AMTA 2018}
\newcommand{\website}{\protect\url{https://sites.google.com/view/loresmt/}}
\newcommand{\contactname}{research track co-chair Yaser Al-Onaizan}
\newcommand{\contactemail}{chaohong.liu@adaptcentre.ie}
\newcommand{\conffilename}{mtsummit2015}
\newcommand{\downloadsite}{\protect\url{http://www.conference.amtaweb.org/}}
\newcommand{\paperlength}{$8$ (eight)}
\newcommand{\shortpaperlength}{$4$ (four)}

%% do not add any other page- or text-size instruction here

\parskip=0.00in

\begin{document}

% \mtsummitHeader{x}{x}{xxx-xxx}{2015}{45-character paper description goes here}{Author(s) initials and last name go here}
\title{\bf System description of Supervised and Unsupervised NMT approaches from {NL~Processing} team at DeepHack.Babel task}
\author{\name{\bf Ilya Gusev} \hfill  \addr{ilya.gusev@phystech.edu}\\
        \addr{MIPT, Dolgoprudny, Moscow Region, 141701, Russian Federation}
\AND
        \name{\bf Artem Oboturov} \hfill \addr{oboturov@gmail.com}
}

\maketitle
\pagestyle{empty}

\begin{abstract}
  A comparison of supervised and unsupervised NMT models was done for the corpora provided by the DeepHack.Babel competition.
  It is shown that for even small parallel corpus, fully supervised NMT gives better results than fully unsupervised for the case of constrained domain of the corpus.
  We have also implemented a recent fully unsupervised NMT model which has given good final results.
\end{abstract}

\section{Introduction}
\label{sect:intro}

We present results obtained by the NL Processing team during the DeepHack.Babel hackathon~\footnote{\url{http://babel.tilda.ws/}} on semi-supervised machine translation.
Organizers of the competition created an oblivious setup - a case in which the source and target languages are not known and the machine translation system can be trained with no specific tuning to the language pair.
The scoring system would run training and then run translation with the trained model.
Participants have no insight into the process and could only observe the final score for a submission and/or the failure status.
Submissions were scored in BLEU-4~\citep{papineni2002bleu}.

For each language pair the following datasets~\footnote{\url{http://contest.deephack.me/c/babel/overview}} were available:
\begin{itemize}
  \item two monolingual corpora for each language pair, {\tt 1M} sentences;
  \item a small parallel corpus, {\tt 50K} sentence pairs;
  \item and an input corpus to be translated from source to target language, {\tt 6K} sentences.
\end{itemize}
There were 3 language pairs used during the competition: {\tt En-Ru} for test, {\tt Lv-En} for qualification and {\tt En-Ko} for final scoring.
More information about corpora is available in Section~\ref{sect:data}.

The machine translation system could be built as a fully supervised one, though the parallel corpus is small ({\tt 50K}); as an unsupervised one, using the two monolingual corpora; and as a semi-supervised one.
Our solutions followed the fully supervised and fully unsupervised approaches only.

Given the problem at hand, the paper devises a number of baseline approaches and then compare them to the unsupervised model described above.
Section~\ref{sect:baselines} will outline the baselines which were used to benchmark the unsupervised NMT in the oblivious setup.
In Section~\ref{sect:unmt} we discuss the experiments with the unsupervised NMT model for the oblivious setup.
Finally, in Section~\ref{sect:nonoblivious} we investigate whether prior knowledge of a language pair gives an advantage for the unsupervised NMT approach.

\section{Baselines}
\label{sect:baselines}

A supervised NMT model~\footnote{For a full description of the Encoder-Decoder architecture see \url{https://github.com/aoboturov/loresmt-nlprocessing\#supervised-model-description}.} was chosen for the baseline.
The model was implemented in OpenNMT~\citep{opennmt} and had the following Encoder-Decoder architecture:
\begin{itemize}
\item encoder is a continuous embedding from the source language to a $300$ dimensional space;
\item followed by a $3$-layers LSTM with a dropout;
\item the decoder has a stacked LSTM with a dropout, a global attention \citep{luong2015effective} and a continuous embedding from a $300$ dimensional space.
\end{itemize}

All those models were trained only on a {\tt 50K} parallel corpus with a {\tt 5\%} validation set.
Training on a NVIDIA Titan XP GPU usually lasted $20$ to $30$ minutes.
The results are provided in Table~\ref{table:baselines}.
Another baseline which is reported here is an input to output copy which is reported as the {\tt no-Epochs} baseline.
Additionally, embeddings were trained with Fasttext~\citep{bojanowski2016enriching}.

\begin{table}
\begin{center}
\begin{tabular}{ l c c c }
Pair & Epochs & Oblivious Score & Conscious Score \\
\hline
\multirow{5}{4em}{En-Ru} & - & 0.02123 & - \\
& 1 & 0.10783 & \\
& 5 & 0.25747 & \\
& 10 & 0.28915 & \\
& Best & -  & 0.298 \\
\hline
\multirow{5}{4em}{Lv-En} & - & 0.02075 & - \\
& 1 & 0.01142 & \\
& 5 & 0.04766 & \\
& 10 & 0.05756 & \\
& Best & - & 0.229 \\
\hline
\multirow{5}{4em}{En-Ko} & - & 0.02759 & - \\
& 1 & 0.11179 & \\
& 5 & 0.22945 & \\
& 10 & 0.25418 & \\
& Best & - & 0.2795
\end{tabular}
\end{center}
\caption{Supervised NMT baselines, measured in BLEU scores.}
\caption*{\small
The best {\tt Lv-En} and  {\tt En-Ru} are reported on newstest2017 corpora in \cite{bojar2017findings}.
% Lv-En \url{http://matrix.statmt.org/matrix/output/1872?score_id=22981}
%\url{http://matrix.statmt.org/matrix/output/1875?score_id=21229}
{\tt En-Ko} is reported after \cite{junczys2016coppa} which uses COPPA corpus.
{\tt Best} here provides an indicative upper boundary on what an MT system trained on a generic parallel corpus might score on a translation task.
}
\label{table:baselines}
\end{table}

On the {\tt Lv-En} langauge pair model performance was mediocre.
It could be explained by the fact that {\tt En-Ru} and {\tt En-Ko} were topic-resticted corpora - both were related to tourism only, while the {\tt Lv-En} corpora was extracted from a news feed which had no topic constraints.

\section{Unsupervised Neural Machine Translation}
\label{sect:unmt}

Competition included not only parallel corpus for each language pair but  {\tt 1M} monolingual corpus for each language.
One way to leverage this data is to use unsupervised NMT model described in \cite{DBLP:journals/corr/abs-1711-00043}.
The code for this model is not available, so we built our own implementation~\footnote{\url{https://github.com/IlyaGusev/UNMT}} based on PyTorch framework. 

Our system can be used in several ways.
First, one can use it like an ordinary supervised NMT system.
Second, it can work as a denoising autoencoder.
Finally, one can train this model on monolingual corpora using a predefined zero model.

There are several types of zero models:
\begin{itemize}
\item word by word model;
\item supervised model trained on a small corpora;
\item unsupervised model from the previous step.
\end{itemize}

In any case, given a zero model, UNMT~\footnote{The UNMT model used for translation is described in \url{https://github.com/aoboturov/loresmt-nlprocessing\#unmt-model-description}} would train iteratively using adversarial training~\citep{goodfellow2016nips} with a discriminator~\footnote{The Discriminator description is available online \url{https://github.com/aoboturov/loresmt-nlprocessing\#unmt-discriminator-description}}.

System showed good results in first two tasks, but failed in the last one.
We don't know whether there is an error in our system or the corpora are too small.
BLEU scores on all language pairs were below {\tt 0.01 BLEU}.
Overall it has an RNN Encoder-Decoder architecture \citep{DBLP:journals/corr/ChoMGBSB14} with continuous embeddings and a global attention \citep{luong2015effective}.

\section{Prior Language Pair Information}
\label{sect:nonoblivious}

Competition was structured in a way to prevent participants from knowing what language pairs were being tested.
The only information available to participants was the BLEU score and the failure or success status for the submission.
One could devise at least two attacks to identify the language pair and then using this prior knowledge, use it to consturct a better translation algorithm.

First we will explore the nature of those attacks and then measure what might be an impact of such knowledge.

\subsection{Side-Channel Attacks against the Language Pair}
\label{sect:attack}

We describe two ways how a language pair identification attack could be executed.

\subsubsection{Using Execution Time}

There is a way to identify the language pair in one submission by using the side-channel attack technique.
In this particular case, the side-channel would be the execution time of the translation algorithm whereby a language identification routine is run on each of the non-parallel corpora and both languages of the pair are detected.
Given that the routine could identify $N$ languages, all the pairs could be enumerated to define a mapping to natural numbers in range $1\dots N*(N-1)$.
Provided that a specific constant delay is used, one could divide the total execution time by the delay duration to obtain the index of the pair in the mapping.

\subsubsection{Using Failure Status}

The second way is a slower combinatorial way where a failure status is used as an indicator of the language belonging to a subset of languages being tested.
A set of all languages identifiable by the routine could be searched in log-time in a breadth search fashion descending only into subsets where we have established an inclusion relationship.

\subsection{Would Prior Knowledge Matter?}
\label{sect:prior}

In Table~\ref{table:baselines} we also report the best BLEU scores available within the conference submissions for each of the pairs trained on common corpora.

We could see that a margin of improvement is just a couple of BLEU points for {\tt En-Ru} and {\tt En-Ko} pairs.
On the other hand, {\tt Lv-En} has a very poor result and we would expect that both unsupervised learning and prior knowledge would improve this score.
%It would make sense to only investigate the {\tt Lv-En} further.

%\todo[inline]{Elaborate on Lv-En language pair}

\section{Conclusion}

A generic fully unsupervised machine translation problem is hard.
In some cases, one could obtain good machine translation models by having a small data set for a limited domain, e.g. for a case of travelling destinations or some other domain-specific translation.
Although semi-supervised translation might improve the results, we have not observed that a fully supervised model used as the zero model for the UNMT made any translation improvement over a regular supervised model.
Poor performance of the UNMT has to be investigated further, possibly by providing larger non-parallel corpora and changing UNMT model architecture.

\section{Data availability and reproducibility}
\label{sect:data}

Data for training and test are not available publicly and organizers would not release them.
Datasets were already summarized in Section~\ref{sect:intro}.

Table~\ref{table:corpora_stats} describes statistics for the corpora.

\begin{table}[h!]
\begin{center}
\begin{tabular}{ l c c c c }
Pair & Source Tokens & Source Words & Target Tokens & Target Words \\
En-Ru & 14M & 165519 & 19M & 345444 \\
Lv-En & 21M & 502858 & 24M & 341012 \\
En-Ko & 14M & 157649 & 7M & 530124 \\
\end{tabular}
\end{center}
\caption{Descriptive statistics for the corpora.}
\label{table:corpora_stats}
\end{table}

Samples from the parallel corpora are provided online \url{https://github.com/aoboturov/loresmt-nlprocessing\#the-corpora-extracts}

Images used to train models are publicly available at \url{https://github.com/aoboturov/loresmt-nlprocessing}.

\small

\bibliographystyle{apalike}
\bibliography{mtsummit2015}

\end{document}

