\documentclass[]{article}
\usepackage[letterpaper]{geometry}
\usepackage{mtsummit2015}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{natbib}
\usepackage{layout}
\usepackage{multirow}
\usepackage[dvipsnames]{xcolor}  % Coloured text etc.
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\usepackage{listings}
\usepackage{color}

\newcommand{\confname}{AMTA 2018}
\newcommand{\website}{\protect\url{https://sites.google.com/view/loresmt/}}
\newcommand{\contactname}{research track co-chair Yaser Al-Onaizan}
\newcommand{\contactemail}{chaohong.liu@adaptcentre.ie}
\newcommand{\conffilename}{mtsummit2015}
\newcommand{\downloadsite}{\protect\url{http://www.conference.amtaweb.org/}}
\newcommand{\paperlength}{$8$ (eight)}
\newcommand{\shortpaperlength}{$4$ (four)}

%% do not add any other page- or text-size instruction here

\parskip=0.00in

\begin{document}

% \mtsummitHeader{x}{x}{xxx-xxx}{2015}{45-character paper description goes here}{Author(s) initials and last name go here}
\title{\bf (Un)Supervised Machine Translation \\
  for the \confname~Conference}
\author{\name{\bf Ilya Gusev} \hfill  \addr{ilya.gusev@phystech.edu}\\
        \addr{MPTI, Dolgoprudny, Moscow Region, 141701, Russian Federation}
\AND
        \name{\bf Artem Oboturov} \hfill \addr{oboturov@gmail.com}
}

\maketitle
\pagestyle{empty}

\begin{abstract}
  A comparision of supervised and unsupervised NMT models was done for the corpora provided by the DeepHack.Babel competition.
  \todo[inline]{Write an abstract}
\end{abstract}

\section{Introduction}

\todo[inline]{Write problem statement}

Given the problem at hand one could devise a number of baseline approaches and then compare them to the unsupervised model described above.
Section~\ref{sec:baselines} outlines the baselines which were used to benchmark the UNMT in the oblivious setup.
In Section~\ref{sec:unmt} we perform experiments with the UNMT model for the oblivious setup.
Finally, in Section~\ref{sec:nonoblivious} we are investigating whether prior knowledge of a language pair gives an advantage for the UNMT approach.

\section{Baselines}
\label{sect:baselines}

A supervised NMT model was chosen for the baseline.
The model was implemented in OpenNMT~\citep{opennmt} and had the following Encoder-Decoder architecture:
\begin{itemize}
\item encoder is a continuous embedding from the source language to a $300$ dimensional space;
\item followed by a $3$-layers LSTM with a dropout;
\item the decoder has a stacked LSTM with a dropout, a global attention \citep{bahdanau2014neural} and a continuous embedding from a $300$ dimensional space,
\end{itemize}
full description is given in an Appendix~\ref{appendix:supervised}.

All those models were trained only on a {\tt 50K} parallel corpus with a {\tt 5\%} validation set.
Training time on a NVIDIA Titan XP GPU was usually measured in tens of minutes.
The results are provided in the Table~\ref{table:baselines}.
Another baseline which is reported here is a input to output copy which is reported as the {\tt no-Epochs} baseline.
Embeddings were trained with Fasttext~\citep{bojanowski2016enriching}.

\begin{table}
\begin{center}
\begin{tabular}{ l c l }
Pair & Epochs & Score \\
\hline
\multirow{3}{4em}{En-Ru} & - & 0.02123 \\
& 1 & 0.10783 \\
& 5 & 0.25747 \\
& 10 & 0.28915 \\
\hline
\multirow{3}{4em}{Lv-En} & - & 0.02075 \\
& 1 & 0.01142 \\
& 5 & 0.04766 \\
& 10 & 0.05756 \\
\hline
\multirow{3}{4em}{En-Ko} & - & 0.02759 \\
& 1 & 0.11179 \\
& 5 & 0.22945 \\
& 10 & 0.25418 \\
\end{tabular}
\end{center}
\caption{Supervised NMT baselines, measured in BLEU scores.}
\label{table:baselines}
\end{table}

One could notice, that on the {\tt Lv-En} langauge pair, model performance was mediocre.
It could be explained by the fact that {\tt En-Ru} and {\tt En-Ko} were topic-resticted corpora - both were descriptions of hotels only, while the {\tt Lv-En} corpora was extracted from a news feed which had no topics constraints.

\section{Unsupervised Neural Machine Translation}
\label{sect:unmt}

\section{Prior Language Pair Information}
\label{sect:nonoblivious}

\subsection{Attacking the Language Pair}
\label{sect:attack}

\subsection{Would Prior Knowledge Matter?}
\label{sect:prior}

\section{Conclusion}

%This text format is derived from that of the Journal of Evolutionary
%Computation.\footnote{Originally written by Darrell Whitley, and only
%  later modified by Marc Schoenauer, especially regarding the
%  bibliography style}

\subsection{Sections}

{\bf Headings}: Type and label section and subsection headings in the
style shown on the present document.  Use numbered sections (Arabic
numerals) in order to facilitate cross references. Number subsections
with the section number and the subsection number separated by a dot,
in Arabic numerals. Do not number subsubsections. Use 9-point bold
font for subsection headings and 11-point bold font for section
headings.

{\bf Citations}: Citations within the text appear in parentheses
as~\citep{Smith} or, if the author's name appears in the text itself,
as \cite{Smith}. Citations in parentheses should not be used as
linguistic phrases; for example, instead of ``\citep{Smith}
\nobreak{argues} that \ldots'' say ``\cite{Smith} \nobreak{argues}
that \ldots''.  Treat double authors as in~\citep{DD15}, but write as
in~\citep{PDC} when more than two authors are involved.
\nobreak{Append} lowercase letters to the year in cases of ambiguity
as in \citep{JonesFirst}.  Collapse multiple citations in parenthesis
as in~\citep{Smith,JonesFirst} and like this for multiple citations
with the same-named \nobreak{author:}
\citep{Smith,SmithConc,JonesFirst,JonesSecond}
%(Tam and Schultz, 2006, 2007; Gledson and Keane, 2008a,b).

\textbf{References}: Gather the full set of references together under the
heading {\bf References}; place the section before any Appendices, unless they
contain references. Arrange the references alphabetically by the first
author's last-name, rather than by order of occurrence in the text, and invert
the first-name and last-name of the first author (only). Provide as complete
a citation as possible, using a consistent format, such as the one for {\em
North American Opera\/}.  Use of full names for
authors rather than initials is preferred.  Use full names for journals and
conferences, not abbreviations (for example ``45th Meeting of the Association
for Computational Linguistics'', not ``ACL07'').

The \LaTeX2e{} and Bib\TeX{} style files provided roughly fit the
American Psychological Association format, allowing regular citations,
short citations and multiple citations as described above.

{\bf Appendices}: Appendices, if any, directly follow the text and the
references (but see above).  Letter them in sequence and provide an
informative title: {\bf Appendix A. Title of Appendix}.

\textbf{Acknowledgement} section should go as a last section immediately
\textit{before the references}.  Do not number the acknowledgement section.

\subsection{Footnotes}

{\bf Footnotes}: Put footnotes at the bottom of the page and use
9-point font. They may be numbered or referred to by asterisks or
other symbols.\footnote{This is how a footnote should appear.}
Footnotes should be separated from the main text by a
line.\footnote{Note the line separating the footnotes from the text.}

\small

\bibliographystyle{apalike}
\bibliography{mtsummit2015}

\begin{appendices}
\section{Supervised NMT Model}
\label{appendix:supervised}
\lstinputlisting{supervised-model.txt}
\end{appendices}

\end{document}
